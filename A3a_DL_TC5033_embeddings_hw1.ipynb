{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigoromanguzman/Actividades_Aprendizaje-/blob/main/A3a_DL_TC5033_embeddings_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6142099",
      "metadata": {
        "id": "c6142099"
      },
      "source": [
        "## TC 5033\n",
        "### Word Embeddings\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 3a: Exploring Word Embeddings with GloVe and Numpy\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - To understand the concept of word embeddings and their significance in Natural Language Processing.\n",
        "    - To learn how to manipulate and visualize high-dimensional data using dimensionality reduction techniques like PCA and t-SNE.\n",
        "    - To gain hands-on experience in implementing word similarity and analogies using GloVe embeddings and Numpy.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Download GloVe pre-trained vectors from the provided link in Canvas, the official public project:\n",
        "    Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation\n",
        "    https://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "    - Create a dictorionay of the embeddings so that you carry out fast look ups. Save that dictionary e.g. as a serialized file for faster loading in future uses.\n",
        "    \n",
        "    - PCA and t-SNE Visualization: After loading the GloVe embeddings, use Numpy and Sklearn to perform PCA and t-SNE to reduce the dimensionality of the embeddings and visualize them in a 2D or 3D space.\n",
        "\n",
        "    - Word Similarity: Implement a function that takes a word as input and returns the 'n' most similar words based on their embeddings. You should use Numpy to implement this function, using libraries that already implement this function (e.g. Gensim) will result in zero points.\n",
        "\n",
        "    - Word Analogies: Implement a function to solve analogies between words. For example, \"man is to king as woman is to ____\". You should use Numpy to implement this function, using libraries that already implement this function (e.g. Gensim) will result in zero points.\n",
        "\n",
        "    - Submission: This activity is to be submitted in teams of 3 or 4. Only one person should submit the final work, with the full names of all team members included in a markdown cell at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "\n",
        "    - Code Quality (40%): Your code should be well-organized, clearly commented, and easy to follow. Use also markdown cells for clarity.\n",
        "    \n",
        "   - Functionality (60%): All functions should work as intended, without errors.\n",
        "       - Visualization of PCA and t-SNE (10% each for a total of 20%)\n",
        "       - Similarity function (20%)\n",
        "       - Analogy function (20%)\n",
        "|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "329dd09a",
      "metadata": {
        "id": "329dd09a"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4af04b9d",
      "metadata": {
        "id": "4af04b9d"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pickle\n",
        "\n",
        "# Libraries for reading file from gdrive\n",
        "from google.colab import drive\n",
        "\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOg5BtGqMuW2",
        "outputId": "10386fc0-5e47-4908-c4a7-57a5fe7679fe"
      },
      "id": "UOg5BtGqMuW2",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8712cf42",
      "metadata": {
        "id": "8712cf42"
      },
      "source": [
        "#### Load file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8c3c9d2a",
      "metadata": {
        "id": "8c3c9d2a"
      },
      "outputs": [],
      "source": [
        "# PATH = '/media/pepe/DataUbuntu/Databases/glove_embeddings/glove.6B.200d.txt'\n",
        "# PATH = '/media/pepe/DataUbuntu/Databases/glove_embeddings/glove.6B.50d.txt'\n",
        "PATH = '/content/drive/My Drive/glove.6B.50d.txt'\n",
        "emb_dim = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "25a1eae6",
      "metadata": {
        "code_folding": [],
        "id": "25a1eae6"
      },
      "outputs": [],
      "source": [
        "# Create dictionary with embeddings\n",
        "def create_emb_dictionary(path):\n",
        "  dictionary = {}\n",
        "  with open(PATH, 'r') as file:\n",
        "    for line in file:\n",
        "        # Split the line where a newline character is found\n",
        "        line = line.strip()  # Remove leading/trailing whitespace, including newline characters\n",
        "        words = line.split() # Split the elements of the line by their space\n",
        "        if len(words)>=2:\n",
        "          key = words[0] #Take the first element which is the word as a key\n",
        "          value =  np.array(words[1:],dtype=float) #Use the rest of the elements as the numpy values\n",
        "          dictionary[key] = value #Store the values in the dictionary\n",
        "  return dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "347a0e5e",
      "metadata": {
        "code_folding": [],
        "id": "347a0e5e"
      },
      "outputs": [],
      "source": [
        "# create dictionary\n",
        "embeddings_dict = create_emb_dictionary(PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>\n",
        "We serialize the embeddings_dict dictionary into a binary file for efficient storage and later retrieval, simplifying the process of accessing the dictionary's data.</h2>"
      ],
      "metadata": {
        "id": "iXFRi5tyU5Q8"
      },
      "id": "iXFRi5tyU5Q8"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1f01b760",
      "metadata": {
        "code_folding": [
          1
        ],
        "id": "1f01b760"
      },
      "outputs": [],
      "source": [
        "# Serialize\n",
        "with open('embeddings_dict_50D.pkl', 'wb') as f:\n",
        "    pickle.dump(embeddings_dict, f)\n",
        "\n",
        "# Deserialize\n",
        "# with open('embeddings_dict_200D.pkl', 'rb') as f:\n",
        "#     embeddings_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0ab9aa",
      "metadata": {
        "id": "fd0ab9aa"
      },
      "source": [
        "#### See some embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8b0991a9",
      "metadata": {
        "id": "8b0991a9"
      },
      "outputs": [],
      "source": [
        "# Show some\n",
        "def show_n_first_words(path, n_words):\n",
        "        with open(path, 'r') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                print(line.split(), len(line.split()[1:]))\n",
        "                if i>=n_words: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a16259ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a16259ec",
        "outputId": "90caa984-db55-4399-9ee1-13ab4965ed56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', '0.418', '0.24968', '-0.41242', '0.1217', '0.34527', '-0.044457', '-0.49688', '-0.17862', '-0.00066023', '-0.6566', '0.27843', '-0.14767', '-0.55677', '0.14658', '-0.0095095', '0.011658', '0.10204', '-0.12792', '-0.8443', '-0.12181', '-0.016801', '-0.33279', '-0.1552', '-0.23131', '-0.19181', '-1.8823', '-0.76746', '0.099051', '-0.42125', '-0.19526', '4.0071', '-0.18594', '-0.52287', '-0.31681', '0.00059213', '0.0074449', '0.17778', '-0.15897', '0.012041', '-0.054223', '-0.29871', '-0.15749', '-0.34758', '-0.045637', '-0.44251', '0.18785', '0.0027849', '-0.18411', '-0.11514', '-0.78581'] 50\n",
            "[',', '0.013441', '0.23682', '-0.16899', '0.40951', '0.63812', '0.47709', '-0.42852', '-0.55641', '-0.364', '-0.23938', '0.13001', '-0.063734', '-0.39575', '-0.48162', '0.23291', '0.090201', '-0.13324', '0.078639', '-0.41634', '-0.15428', '0.10068', '0.48891', '0.31226', '-0.1252', '-0.037512', '-1.5179', '0.12612', '-0.02442', '-0.042961', '-0.28351', '3.5416', '-0.11956', '-0.014533', '-0.1499', '0.21864', '-0.33412', '-0.13872', '0.31806', '0.70358', '0.44858', '-0.080262', '0.63003', '0.32111', '-0.46765', '0.22786', '0.36034', '-0.37818', '-0.56657', '0.044691', '0.30392'] 50\n",
            "['.', '0.15164', '0.30177', '-0.16763', '0.17684', '0.31719', '0.33973', '-0.43478', '-0.31086', '-0.44999', '-0.29486', '0.16608', '0.11963', '-0.41328', '-0.42353', '0.59868', '0.28825', '-0.11547', '-0.041848', '-0.67989', '-0.25063', '0.18472', '0.086876', '0.46582', '0.015035', '0.043474', '-1.4671', '-0.30384', '-0.023441', '0.30589', '-0.21785', '3.746', '0.0042284', '-0.18436', '-0.46209', '0.098329', '-0.11907', '0.23919', '0.1161', '0.41705', '0.056763', '-6.3681e-05', '0.068987', '0.087939', '-0.10285', '-0.13931', '0.22314', '-0.080803', '-0.35652', '0.016413', '0.10216'] 50\n",
            "['of', '0.70853', '0.57088', '-0.4716', '0.18048', '0.54449', '0.72603', '0.18157', '-0.52393', '0.10381', '-0.17566', '0.078852', '-0.36216', '-0.11829', '-0.83336', '0.11917', '-0.16605', '0.061555', '-0.012719', '-0.56623', '0.013616', '0.22851', '-0.14396', '-0.067549', '-0.38157', '-0.23698', '-1.7037', '-0.86692', '-0.26704', '-0.2589', '0.1767', '3.8676', '-0.1613', '-0.13273', '-0.68881', '0.18444', '0.0052464', '-0.33874', '-0.078956', '0.24185', '0.36576', '-0.34727', '0.28483', '0.075693', '-0.062178', '-0.38988', '0.22902', '-0.21617', '-0.22562', '-0.093918', '-0.80375'] 50\n",
            "['to', '0.68047', '-0.039263', '0.30186', '-0.17792', '0.42962', '0.032246', '-0.41376', '0.13228', '-0.29847', '-0.085253', '0.17118', '0.22419', '-0.10046', '-0.43653', '0.33418', '0.67846', '0.057204', '-0.34448', '-0.42785', '-0.43275', '0.55963', '0.10032', '0.18677', '-0.26854', '0.037334', '-2.0932', '0.22171', '-0.39868', '0.20912', '-0.55725', '3.8826', '0.47466', '-0.95658', '-0.37788', '0.20869', '-0.32752', '0.12751', '0.088359', '0.16351', '-0.21634', '-0.094375', '0.018324', '0.21048', '-0.03088', '-0.19722', '0.082279', '-0.09434', '-0.073297', '-0.064699', '-0.26044'] 50\n",
            "['and', '0.26818', '0.14346', '-0.27877', '0.016257', '0.11384', '0.69923', '-0.51332', '-0.47368', '-0.33075', '-0.13834', '0.2702', '0.30938', '-0.45012', '-0.4127', '-0.09932', '0.038085', '0.029749', '0.10076', '-0.25058', '-0.51818', '0.34558', '0.44922', '0.48791', '-0.080866', '-0.10121', '-1.3777', '-0.10866', '-0.23201', '0.012839', '-0.46508', '3.8463', '0.31362', '0.13643', '-0.52244', '0.3302', '0.33707', '-0.35601', '0.32431', '0.12041', '0.3512', '-0.069043', '0.36885', '0.25168', '-0.24517', '0.25381', '0.1367', '-0.31178', '-0.6321', '-0.25028', '-0.38097'] 50\n"
          ]
        }
      ],
      "source": [
        "show_n_first_words(PATH, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff828123",
      "metadata": {
        "id": "ff828123"
      },
      "source": [
        "### Plot some embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "3d3ffaa4",
      "metadata": {
        "id": "3d3ffaa4"
      },
      "outputs": [],
      "source": [
        "def plot_embeddings(emb_path, words2show, emb_dim, embeddings_dict, func=PCA):\n",
        "    # Perform dimensionality reduction with dimensionReduce to 2 dimensions\n",
        "    dimensionReduce = func(n_components=2)\n",
        "    transformed_data = dimensionReduce.fit_transform(np.array(list(embeddings_dict.values())))\n",
        "\n",
        "    # Create a dictionary with the transformed data\n",
        "    transformed_dict = {word: transformed_data[i] for i, word in enumerate(embeddings_dict.keys())}\n",
        "\n",
        "    # Scatter plot the transformed data in 2D\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plot_values = np.array([transformed_dict[word] for word in words2show])\n",
        "    plt.scatter(plot_values[:, 0], plot_values[:, 1], marker='o', s=50)\n",
        "\n",
        "    # Add labels to the points\n",
        "    for word in words2show:\n",
        "        plt.annotate(word, transformed_dict[word], fontsize=10)\n",
        "\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.title('dimensionReduce Plot in 2D')\n",
        "    plt.show()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "9b1b120a",
      "metadata": {
        "id": "9b1b120a"
      },
      "outputs": [],
      "source": [
        "words= ['burger', 'tortilla', 'bread', 'pizza', 'beef', 'steak', 'fries', 'chips',\n",
        "            'argentina', 'mexico', 'spain', 'usa', 'france', 'italy', 'greece', 'china',\n",
        "            'water', 'beer', 'tequila', 'wine', 'whisky', 'brandy', 'vodka', 'coffee', 'tea',\n",
        "            'apple', 'banana', 'orange', 'lemon', 'grapefruit', 'grape', 'strawberry', 'raspberry',\n",
        "            'school', 'work', 'university', 'highschool']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d33789",
      "metadata": {
        "id": "87d33789"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_embeddings(PATH, words, emb_dim, embeddings_dict, PCA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22bacb4",
      "metadata": {
        "id": "f22bacb4"
      },
      "outputs": [],
      "source": [
        "# t-SNE dimensionality reduction for visualization\n",
        "embeddings = plot_embeddings(PATH, words, emb_dim, embeddings_dict, TSNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXcutV7A3FHy"
      },
      "id": "vXcutV7A3FHy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8211b7f1",
      "metadata": {
        "id": "8211b7f1"
      },
      "source": [
        "### Let us compute analogies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0003ce18",
      "metadata": {
        "id": "0003ce18"
      },
      "outputs": [],
      "source": [
        "# analogy\n",
        "def analogy(word1, word2, word3, embeddings_dict):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c02ec01",
      "metadata": {
        "id": "9c02ec01"
      },
      "outputs": [],
      "source": [
        "analogy('man', 'king', 'woman', embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb50e0f7",
      "metadata": {
        "id": "bb50e0f7"
      },
      "outputs": [],
      "source": [
        "# most similar\n",
        "def find_most_similar(word, embeddings_dict, top_n=10):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7946d712",
      "metadata": {
        "id": "7946d712"
      },
      "outputs": [],
      "source": [
        "most_similar = find_most_similar('mexico', embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b6e0ea1",
      "metadata": {
        "id": "3b6e0ea1"
      },
      "outputs": [],
      "source": [
        "for i, w in enumerate(most_similar, 1):\n",
        "    print(f'{i} ---> {w[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09875cae",
      "metadata": {
        "id": "09875cae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985bfda4",
      "metadata": {
        "id": "985bfda4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06f33c5",
      "metadata": {
        "id": "f06f33c5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e86fa7",
      "metadata": {
        "id": "05e86fa7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e1838b",
      "metadata": {
        "id": "e1e1838b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1a9fad",
      "metadata": {
        "id": "fe1a9fad"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}