{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigoromanguzman/Actividades_Aprendizaje-/blob/main/A4_DL_TC5033_text_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037e89c8",
      "metadata": {
        "id": "037e89c8"
      },
      "source": [
        "## TC 5033\n",
        "### Text Generation\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
        "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
        "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
        "\n",
        "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
        "\n",
        "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n",
        "\n",
        "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
        "\n",
        "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
        "\n",
        "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
        "\n",
        "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
        "\n",
        "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n",
        "\n",
        "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Install the Python libraries that we will need</h3>"
      ],
      "metadata": {
        "id": "ortY_mVkP1TO"
      },
      "id": "ortY_mVkP1TO"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iPQ092qmERp",
        "outputId": "4db7cfe9-6956-4b86-aa2f-703955116c5f"
      },
      "id": "4iPQ092qmERp",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErdCGc3JmcYd",
        "outputId": "2641ed77-167e-48d0-fc10-b4019cc3628e"
      },
      "id": "ErdCGc3JmcYd",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchdata) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchdata) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchdata) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==1.9.1+cu102 torchvision==0.10.1+cu102 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg_sYlFgmnU4",
        "outputId": "068c71d5-38d5-4a49-a179-182867cec76b"
      },
      "id": "Bg_sYlFgmnU4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.1+cu102 (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 1.13.1+rocm5.1.1, 1.13.1+rocm5.2, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.0+rocm5.3, 2.0.0+rocm5.4.2, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2, 2.1.0, 2.1.0+cpu, 2.1.0+cpu.cxx11.abi, 2.1.0+cu118, 2.1.0+cu121, 2.1.0+cu121.with.pypi.cudnn, 2.1.0+rocm5.5, 2.1.0+rocm5.6, 2.1.1, 2.1.1+cpu, 2.1.1+cpu.cxx11.abi, 2.1.1+cu118, 2.1.1+cu121, 2.1.1+cu121.with.pypi.cudnn, 2.1.1+rocm5.5, 2.1.1+rocm5.6)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.9.1+cu102\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkS8gXuNmup6",
        "outputId": "f98af3a1-c7cb-4045-9edf-bde6eebdb6f9"
      },
      "id": "lkS8gXuNmup6",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-plot\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.2.2)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.11.3)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.10/dist-packages (from scikit-plot) (1.3.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Import the lbraries that we will be using</h3>"
      ],
      "metadata": {
        "id": "EI4PWo2pPq0J"
      },
      "id": "EI4PWo2pPq0J"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3eb4b117",
      "metadata": {
        "id": "3eb4b117"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#PyTorch libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "# Dataloader library\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "# Libraries to prepare the data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# neural layers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6d8ff971",
      "metadata": {
        "id": "6d8ff971"
      },
      "outputs": [],
      "source": [
        "# Select the correct device where our app will be running\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f3288ce5",
      "metadata": {
        "id": "f3288ce5"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset = WikiText2()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>WikiText2 dataset</h3>\n",
        "<p>WikiText2 is a large collection of articles taken from Wikipedia, chosen for their quality and breadth of content. The articles included in the dataset are among the higher-quality content available on Wikipedia, having undergone thorough review and editing processes. WikiText-2 is smaller than some other datasets like WikiText-103, making it more manageable for training models with limited computational resources</p>\n",
        "<p>WikiText-2 is typically divided into training, validation, and test sets. The training set is used to train language models.</p>"
      ],
      "metadata": {
        "id": "t7im85qOQemp"
      },
      "id": "t7im85qOQemp"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fc4c7dbd",
      "metadata": {
        "id": "fc4c7dbd"
      },
      "outputs": [],
      "source": [
        "# Set up tokenizer and define a generator function\n",
        "tokeniser = get_tokenizer('basic_english')\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokeniser(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2c2cb068",
      "metadata": {
        "id": "2c2cb068"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "#set unknown token at position 0\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Data processing</h3>\n",
        "<p>Prepare data training, validation and testing. The tensor is split into input (x) and target (y) sequences for the model. For each position i in the input sequence, the corresponding target is the token at position i+1. This setup trains the model to predict the next token in a sequence</p>"
      ],
      "metadata": {
        "id": "V-hZLJ0RTQjL"
      },
      "id": "V-hZLJ0RTQjL"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "134b832b",
      "metadata": {
        "id": "134b832b"
      },
      "outputs": [],
      "source": [
        "seq_length = 50\n",
        "def data_process(raw_text_iter, seq_length = 50):\n",
        "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
        "#     target_data = torch.cat(d)\n",
        "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length),\n",
        "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))\n",
        "\n",
        "# # Create tensors for the training set\n",
        "x_train, y_train = data_process(train_dataset, seq_length)\n",
        "x_val, y_val = data_process(val_dataset, seq_length)\n",
        "x_test, y_test = data_process(test_dataset, seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>Step for preparing data to use with PyTorch's data loading utilities, specifically for batching and loading the data during the training and evaluation process.</p>"
      ],
      "metadata": {
        "id": "1FdEaBcsURAh"
      },
      "id": "1FdEaBcsURAh"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4b54c04d",
      "metadata": {
        "id": "4b54c04d"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "test_dataset = TensorDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p>The DataLoader takes a dataset and provides an iterator that returns batches of data. This is essential for efficiently processing large datasets and for functionalities like shuffling and parallel data loading</p>"
      ],
      "metadata": {
        "id": "5ptli9ryVGQ5"
      },
      "id": "5ptli9ryVGQ5"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f4d400fb",
      "metadata": {
        "id": "f4d400fb"
      },
      "outputs": [],
      "source": [
        "batch_size = 64  # choose a batch size that fits your computation resources\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>LSTM neural network model</h3>\n",
        "<p>The model initializes an embedding layer to convert token indices into dense vector representations, followed by an LSTM layer for processing these sequences. The model's architecture includes a specified number of LSTM layers and a hidden state size, allowing it to capture temporal dependencies within the data. The output of the LSTM is then passed through a linear layer to map it to the size of the vocabulary, facilitating the prediction of the next token in a sequence. Additionally, the model includes a method to initialize the hidden states of the LSTM, ensuring they are reset appropriately for each new batch of data. The model is instantiated with specific parameters like vocabulary size, embedding size, hidden layer size, and the number of LSTM layers, making it suitable for various NLP tasks that involve sequential data</p>"
      ],
      "metadata": {
        "id": "ZqBer5ZQVqUl"
      },
      "id": "ZqBer5ZQVqUl"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "59c63b01",
      "metadata": {
        "id": "59c63b01"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "# Feel free to experiment\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Transforms token indices into dense vectors of fixed size embed_size\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        # size of the hidden state in the LSTM\n",
        "        self.hidden_size = hidden_size\n",
        "        # number of LSTM layers stacked on top of each other.\n",
        "        self.num_layers = num_layers\n",
        "        # LSTM layer that processes sequences of embeddings.\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)# batch_first=True argument indicates that the input and output tensors are provided as (batch, seq, feature)\n",
        "        # fully connected layer that maps the LSTM's output to the size of the vocabulary\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, text, hidden):\n",
        "        embeddings = self.embeddings(text)\n",
        "        # LSTM layer takes the embeddings and the initial hidden state as input, and returns the output for each time step and the final hidden state\n",
        "        output, hidden = self.lstm(embeddings, hidden)\n",
        "        # Output from the LSTM is passed through the linear layer to predict the next token\n",
        "        decoded = self.fc(output)\n",
        "        return decoded, hidden\n",
        "\n",
        "    # Initializes the hidden state and cell state for the LSTM with zeros.\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Ensures that the hidden states are on the same device as the model\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "vocab_size = len(vocab) # vocabulary size\n",
        "emb_size = 200 # embedding size\n",
        "neurons = 200 # the dimension of the feedforward network model, i.e. # of neurons\n",
        "num_layers = 2 # the number of nn.LSTM layers\n",
        "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Testing our model</h3>\n",
        "<p>Given that we have corresponding target sequences for each input data sequence, we are equipped to evaluate the accuracy of our language model. This can be achieved through the test_model function, which is structured to assess the model's performance in a testing environment. The function sets the model to evaluation mode, ensuring that operations like dropout or batch normalization are adjusted appropriately for testing. During evaluation, the model's predictions are compared against the actual target sequences to compute both the average loss and the <b>accuracy</b>.</p>"
      ],
      "metadata": {
        "id": "2RtK7bVJcYkS"
      },
      "id": "2RtK7bVJcYkS"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, criterion, device, batch_size, vocab_size):\n",
        "  model.eval()  # Set the model to evaluation mode\n",
        "  total_loss, total_predictions, correct_predictions = 0, 0, 0\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x_test, y_test in test_loader:\n",
        "      x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "      output, hidden = model(x_test, hidden)\n",
        "      hidden = (hidden[0].detach(), hidden[1].detach())  # Detaching hidden state\n",
        "\n",
        "      loss = criterion(output.view(-1, vocab_size), y_test.view(-1))\n",
        "      total_loss += loss.item() * x_test.size(0)\n",
        "\n",
        "      # Calculate the total number of sequence elements\n",
        "      total_predictions += x_test.size(0) * seq_length\n",
        "\n",
        "      # Calculate accuracy\n",
        "      _, predicted = torch.max(output, 2)  # Get the index of the max log-probability\n",
        "\n",
        "      correct_predictions += (predicted == y_test).view(-1).sum().item()\n",
        "\n",
        "  average_loss = total_loss / total_predictions\n",
        "  accuracy = correct_predictions / total_predictions\n",
        "  return average_loss, accuracy"
      ],
      "metadata": {
        "id": "n-K3kKDPqX02"
      },
      "id": "n-K3kKDPqX02",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training Function with PyTorch</h3>\n",
        "<p>This function serves as a standard training approach for PyTorch-based models. It ensures that the model, the data and the hidden layers are available on the specified device, facilitating GPU acceleration if available.</p>"
      ],
      "metadata": {
        "id": "JIq_2oX_at8L"
      },
      "id": "JIq_2oX_at8L"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "215eabb9",
      "metadata": {
        "id": "215eabb9"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, optimiser):\n",
        "  '''\n",
        "  The following are possible instructions you may want to conside for this function.\n",
        "  This is only a guide and you may change add or remove whatever you consider appropriate\n",
        "  as long as you train your model correctly.\n",
        "      - loop through specified epochs\n",
        "      - loop through dataloader\n",
        "      - don't forget to zero grad!\n",
        "      - place data (both input and target) in device\n",
        "      - init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
        "      - run the model\n",
        "      - compute the cost or loss\n",
        "      - backpropagation\n",
        "      - Update paratemers\n",
        "      - Include print all the information you consider helpful\n",
        "\n",
        "  '''\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  model = model.to(device=device)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for i, (data, labels) in enumerate((train_loader)):\n",
        "      model.train()\n",
        "      # Clear old gradients\n",
        "      optimiser.zero_grad()\n",
        "\n",
        "      # Initialize hidden states for each batch\n",
        "      hidden = model.init_hidden(batch_size)\n",
        "      hidden = (hidden[0].to(device), hidden[1].to(device))\n",
        "\n",
        "      # Convert labels to torch.long data type\n",
        "      inputs = data.to(device = device,dtype=torch.long)\n",
        "      labels = labels.to(device = device,dtype=torch.long)# Move hidden states to device\n",
        "\n",
        "      # Forward pass\n",
        "      outputs, hidden = model(inputs,hidden)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = criterion(outputs.view(-1, vocab_size), labels.view(-1))\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "    average_loss, accuracy = test_model(model, val_loader, criterion, device, batch_size, vocab_size)\n",
        "    print(f\"Average Test Loss: {average_loss}\")\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Training of the model</h3>"
      ],
      "metadata": {
        "id": "W-bzKnvnbcVt"
      },
      "id": "W-bzKnvnbcVt"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "aa9c84ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9c84ce",
        "outputId": "78ec04da-a709-4ee0-b8ab-fc3f48c41ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Test Loss: 0.1134851448571504\n",
            "Test Accuracy: 20.10%\n",
            "Average Test Loss: 0.10986625116262863\n",
            "Test Accuracy: 21.00%\n",
            "Average Test Loss: 0.10804870363491685\n",
            "Test Accuracy: 21.68%\n",
            "Average Test Loss: 0.10686180655636004\n",
            "Test Accuracy: 21.77%\n",
            "Average Test Loss: 0.10594809717206813\n",
            "Test Accuracy: 22.16%\n",
            "Average Test Loss: 0.10599816137285376\n",
            "Test Accuracy: 22.17%\n",
            "Average Test Loss: 0.10565218726200844\n",
            "Test Accuracy: 22.44%\n",
            "Average Test Loss: 0.10621451463272323\n",
            "Test Accuracy: 22.16%\n",
            "Average Test Loss: 0.10567238238320421\n",
            "Test Accuracy: 22.51%\n",
            "Average Test Loss: 0.10613417298046511\n",
            "Test Accuracy: 22.44%\n",
            "Average Test Loss: 0.10573331590908677\n",
            "Test Accuracy: 22.71%\n",
            "Average Test Loss: 0.10613781388126202\n",
            "Test Accuracy: 22.82%\n",
            "Average Test Loss: 0.10680383269466572\n",
            "Test Accuracy: 22.49%\n",
            "Average Test Loss: 0.1074395068723764\n",
            "Test Accuracy: 22.45%\n",
            "Average Test Loss: 0.1079189089874723\n",
            "Test Accuracy: 22.20%\n",
            "Average Test Loss: 0.10819200786192025\n",
            "Test Accuracy: 22.32%\n",
            "Average Test Loss: 0.10876187879647782\n",
            "Test Accuracy: 22.29%\n",
            "Average Test Loss: 0.10939692241042408\n",
            "Test Accuracy: 22.08%\n",
            "Average Test Loss: 0.10998762842434556\n",
            "Test Accuracy: 22.12%\n",
            "Average Test Loss: 0.11054061747308987\n",
            "Test Accuracy: 22.12%\n"
          ]
        }
      ],
      "source": [
        "# Call the train function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.0005\n",
        "epochs = 20\n",
        "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
        "train(model, epochs, optimiser)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c8667411",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8667411",
        "outputId": "375d756d-f552-493b-e583-1f4a26e1644a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like poker things , as the uí king of schopenhauer changed them to his life following hanging for still 10 her squadron . bulloch became released and 2001 attempts to become embroiled in love , and dumah , journal of munster , in great , he did become from dangerously in support of his teens to 45 – 40 by 2 , tennessee was marketed as manager michael <unk> pickering ) , and the champions producers in august ( 97 ) , acting on the oricon and 1980s an elaborate <unk> family guy for the revival . development and queen elizabeth\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, start_text, num_words, temperature=1.0):\n",
        "  model.eval()\n",
        "  words = tokeniser(start_text)\n",
        "  hidden = model.init_hidden(1)\n",
        "  for i in range(0, num_words):\n",
        "      x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
        "      y_pred, hidden = model(x, hidden)\n",
        "      last_word_logits = y_pred[0][-1]\n",
        "      p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
        "      word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "      words.append(vocab.lookup_token(word_index))\n",
        "\n",
        "  return ' '.join(words)\n",
        "\n",
        "\n",
        "# Generate some text\n",
        "print(generate_text(model, start_text=\"I like\", num_words=100))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}