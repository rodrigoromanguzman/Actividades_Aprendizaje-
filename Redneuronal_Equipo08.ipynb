{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigoromanguzman/Actividades_Aprendizaje-/blob/main/Redneuronal_Equipo08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Maestría en Inteligencia Artificial Aplicada**\n",
        "##**Curso: Inteligencia Artificial y Aprendizaje Automático**\n",
        "###Tecnológico de Monterrey\n",
        "###Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Adtividad de la Semana 7**\n",
        "###**Red Neuronal Artificial - Perceptrón Multicapa : Multilayer Perceptrón (MLP)**\n"
      ],
      "metadata": {
        "id": "VFj0sSM06dYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "*   Adrián Alejandro Dávila González - A01039334\n",
        "*   Juan Antonio Melendres Villa - A00369017\n",
        "*   Andrea Margarita Osorio González - A01104776\n",
        "*   Rodrigo Ildefonso Román Guzmán - A01794225\n",
        "*   Jeanette Ríos Martínez - A01688888"
      ],
      "metadata": {
        "id": "Qgrvy0RGB9XI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cada sección deberás incluir todas las líneas de código necesarias para responder a cada uno de los ejercicios."
      ],
      "metadata": {
        "id": "FrJ2ahMODVj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluye aquí todos módulos, librerías y paquetes que requieras.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.dummy import DummyRegressor\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
        "\n",
        "from sklearn.metrics import classification_report, make_scorer\n",
        "from sklearn.model_selection import cross_validate, RepeatedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.inspection import permutation_importance"
      ],
      "metadata": {
        "id": "exXsscs-Dh-2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25brD-gQdZM"
      },
      "source": [
        "#**Ejercicio-1.** "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3nU2GuWYCy6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4cda89-1db8-47a4-ad3e-57c14dc2cb3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "DIR = '/content/drive/MyDrive/Maestría/Segundo Trimestre/Inteligencia Artificial y Aprendizaje Automático/Semana 7'\n",
        "os.chdir(DIR)"
      ],
      "metadata": {
        "id": "0ulkqXVGCy97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "8c536e18-f266-40be-d045-bc92967de7ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-72a8d8b38fab>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Maestría/Segundo Trimestre/Inteligencia Artificial y Aprendizaje Automático/Semana 7'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Maestría/Segundo Trimestre/Inteligencia Artificial y Aprendizaje Automático/Semana 7'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lectura del archivo .csv para crear el dataframe\n",
        "Fb_df = pd.read_csv('dataset_Facebook.csv', sep=';', header= 'infer')\n",
        "Fb_df.head()"
      ],
      "metadata": {
        "id": "MJr4StmqSugc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ajuste del tipo de dato de la variable 'Type' para cambiar de String a Numérico como el resto del conjunto\n",
        "#-Importante debido a que es una de las variables de entrada a considerar para el modelo\n",
        "\n",
        "#Debido al uso del LabelEncoder(), las nuevas etiquetas quedan de la siguiente forma:\n",
        "# 0 - Link\n",
        "# 1 - Photo\n",
        "# 2 - Status\n",
        "# 3 - Video\n",
        "\n",
        "lab_enc= LabelEncoder()\n",
        "Fb_df['Type']= lab_enc.fit_transform(Fb_df['Type'])\n",
        "Fb_df.head()"
      ],
      "metadata": {
        "id": "r0FNVW_fRRzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición de los nombres correctos de las columnas\n",
        "col_names= {'Lifetime People who have liked your Page and engaged with your post' : 'LPE'}\n",
        "\n",
        "#Renombrado de las columnas a su valor correcto\n",
        "Fb_df.rename(columns= col_names, inplace=True)\n",
        "Fb_df.head()"
      ],
      "metadata": {
        "id": "bLYz8rR1tf7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generación de Dataframe mis_datos\n",
        "mis_datos=Fb_df[['Category', 'Page total likes', 'Type', 'Post Month', 'Post Hour', 'Post Weekday', 'Paid','LPE']]\n",
        "mis_datos.head()"
      ],
      "metadata": {
        "id": "3e_TSMi4NXIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separación de variables y generación de dataframes de predictores y respuesta\n",
        "X = mis_datos[['Category', 'Page total likes', 'Type', 'Post Month', 'Post Hour', 'Post Weekday', 'Paid']]\n",
        "y = mis_datos[['LPE']]"
      ],
      "metadata": {
        "id": "zoOcVOYrxJI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversión del tipo de dato de la variable de salida para ajustarse a flotante en lugar de entero\n",
        "y= y.astype(np.float64)\n",
        "y.info()"
      ],
      "metadata": {
        "id": "3hVEJgSiD1jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-2.**"
      ],
      "metadata": {
        "id": "xZhr2hkECzVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Partición de los datos en:\n",
        "# - Conjunto de entrenamiento en 80% (400 datos)\n",
        "# - Conjunto de prueba en 20% (100 datos)\n",
        "\n",
        "#Definición y ejecución de la función 'train_test_split()'\n",
        "X_train_v, X_test, y_train_v, y_test = train_test_split(X, y, test_size=0.2, shuffle= True)\n",
        "print('\\033[1m' + 'Forma de X_train_v:' + '\\033[0m', X_train_v.shape)\n",
        "print('\\033[1m' + 'Forma de y_train_v:' + '\\033[0m', y_train_v.shape)\n",
        "print('\\033[1m' + 'Forma de X_test:' + '\\033[0m', X_test.shape)\n",
        "print('\\033[1m' + 'Forma de y_test:' + '\\033[0m', y_test.shape)"
      ],
      "metadata": {
        "id": "kGfAoOPkC1PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_v.info()"
      ],
      "metadata": {
        "id": "VvVzJNvXEm3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-3.**"
      ],
      "metadata": {
        "id": "NCunuooTC2W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de función para calcular la raíz cuadrada del error cuadrático medio (RMSE)\n",
        "def mi_RMSE(y_val,yhatVal):\n",
        "  rmse = pow(np.mean(np.square(y_val-yhatVal), axis=0),0.5)\n",
        "  return rmse\n",
        "\n",
        "# Definición de función para calcular el error absoluto medio (MAE)\n",
        "def mi_MAE(y_val,yhatVal):\n",
        "  mae = np.mean(np.absolute(y_val-yhatVal), axis=0)\n",
        "  return mae\n",
        "\n",
        "# Definición de función para calcular el error porcentual absoluto medio (MAPE)\n",
        "def mi_MAPE(y_val,yhatVal):\n",
        "  epsilon= np.finfo(float).eps\n",
        "  mape = np.mean(np.absolute((y_val-yhatVal)/(y_val+epsilon)), axis=0)*100\n",
        "  return mape"
      ],
      "metadata": {
        "id": "YXlcSWA-C4Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-4.**"
      ],
      "metadata": {
        "id": "chqk9jIDC5Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Separación de variables en conjuntos de atributos en base a tipo de dato\n",
        "categorical_attributes = ['Category', 'Type', 'Paid']\n",
        "numerical_attributes = ['Page total likes', 'Post Month', 'Post Hour', 'Post Weekday']"
      ],
      "metadata": {
        "id": "RBVSFwK4C6g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Análisis de las variables numéricas para verificar las transformaciones que se requieren\n",
        "\n",
        "#Histograma de 'Page total likes'\n",
        "sns.set(rc={'figure.figsize':(6,4)})\n",
        "X_train_v[\"Page total likes\"].hist(bins=18)"
      ],
      "metadata": {
        "id": "oumK6zIe9LO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Diagrama de caja de 'Page total likes'\n",
        "plt.boxplot(X_train_v['Page total likes'], vert=False, showmeans=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uA_z5dx6-MBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformación a aplicar en 'Page total likes'\n",
        "\n",
        "ptl_max= X_train_v['Page total likes'].max()\n",
        "np.power(ptl_max+ 1-X_train_v['Page total likes'], 0.1).hist(bins=20)"
      ],
      "metadata": {
        "id": "t7Dj4S1X-MbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis**\n",
        "\n",
        "Debido a que el histograma de 'Page total likes' muestra un sesgo negativo, además de tener un rango de valores mayor al de las otras variables de entrada, se le debe aplicar una transformación basada en su potencia.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "4m8O-TKYYEyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Histograma de 'Page Month'\n",
        "X_train_v[\"Post Month\"].hist(bins=23)"
      ],
      "metadata": {
        "id": "ka4booh7-Msx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis**\n",
        "\n",
        "Debido a que el rango de 'Post Month' no se expande a valores tan diferentes de las demás variables y su comportamiento es relativamente similar a una distribución normal, no requiere una transformación específica para ajustarla.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cPIg7dCqXXz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Histograma de 'Page Weekday'\n",
        "X_train_v[\"Post Weekday\"].hist(bins=13)"
      ],
      "metadata": {
        "id": "h9FSmJYP-m5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis**\n",
        "\n",
        "Debido a que el rango de 'Post Weekday' tampoco se expande a valores tan diferentes de las demás variables y su comportamiento es relativamente similar a una distribución normal, no requiere una transformación específica para ajustarla.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sbnWRFQQYU8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Histograma de 'Page Hour'\n",
        "X_train_v[\"Post Hour\"].hist(bins=50)"
      ],
      "metadata": {
        "id": "SvWU40nY-j2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis**\n",
        "\n",
        "Debido a que 'Post Hour' no muestra valores de rango tan extensos y muestra un comportamiento similar a una distribución acampanada, puede dejarse de esta manera sin transformar. \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "BY21LoSUYX9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Histograma de 'LPE'\n",
        "y_train_v['LPE'].hist(bins=20)"
      ],
      "metadata": {
        "id": "81La1LPbVJE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformación a aplicar en 'LPE'\n",
        "\n",
        "np.log(1 + y_train_v['LPE']).hist(bins=20)"
      ],
      "metadata": {
        "id": "pc4wO-NhctGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis**\n",
        "\n",
        "Debido a que 'LPE' es la variable de salida, solamente se le ajusta el sesgo. No se requiere ajustar el rango.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Fzh2-LIQYZjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Análisis de las variables categóricas para confirmar las transformaciones requeridas\n",
        "\n",
        "#Conteo total de Valores en 'Category'\n",
        "X_train_v['Category'].value_counts()"
      ],
      "metadata": {
        "id": "pfk_oLkHXBTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtención del porcentaje de cada elemento con respecto al total\n",
        "\n",
        "X_train_v['Category'].value_counts()/X_train_v.shape[0]"
      ],
      "metadata": {
        "id": "QnTv2pnxYTuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis**\n",
        "\n",
        "En el caso de la variable 'Category', se puede apreciar que cada una de las etiquetas cuenta con una representación por encima del 5% del total en el conjunto de datos, por lo cual se pueden tomar en cuenta sin necesidad de agrupar elementos. \n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "e-Z6uEB9Yb1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Conteo total de Valores en 'Paid'\n",
        "\n",
        "X_train_v['Paid'].value_counts()"
      ],
      "metadata": {
        "id": "w_tdkGnFYURf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtención del porcentaje de cada elemento con respecto al total\n",
        "\n",
        "X_train_v['Paid'].value_counts()/X_train_v.shape[0]"
      ],
      "metadata": {
        "id": "XJMtecDEYVuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis**\n",
        "\n",
        "De igual forma, en la variable 'Paid', se puede apreciar que cada una de las etiquetas cuenta con una representación por encima del 5% del total en el conjunto de datos, por lo cual se pueden tomar en cuenta sin necesidad de agrupar elementos.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yqnCg-VtYdN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Conteo total de Valores en 'Type'\n",
        "\n",
        "X_train_v['Type'].value_counts()"
      ],
      "metadata": {
        "id": "U4Sv8IlYYW14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtención del porcentaje de cada elemento con respecto al total\n",
        "\n",
        "X_train_v['Type'].value_counts()/X_train_v.shape[0]"
      ],
      "metadata": {
        "id": "RP_dE65rYYKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis**\n",
        "\n",
        "Debido a que dos de las etiquetas de 'Type' se encuentran en un valor por debajo del mínimo porcentaje de representación, se van a unir en una nueva categoría para poder cumplir con el criterio del mínimo de información.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "NK7z_rVlYemo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición de las funciones para transformar 'Page total likes', 'LPE' y 'Type'\n",
        "\n",
        "#Transformador de Potencia\n",
        "def mi_power(x):\n",
        "  x_max=np.max(x)\n",
        "  t= np.power(x_max + 1 - x, 0.1)\n",
        "  return t\n",
        "\n",
        "#Transformador de Logaritmo natural\n",
        "def mi_log(y):\n",
        "  t= np.log(1+y)\n",
        "  return t\n",
        "\n",
        "#Tranformado de Logaritmo natural inverso\n",
        "# *Opcional, si se quisiera obtener las predicciones y la salida en unidades originales\n",
        "def mi_loginv(y):\n",
        "  t= np.exp(y) - 1\n",
        "  return t \n",
        "\n",
        "#Definición de la agrupación nueva para la variable Type\n",
        "def mi_type(x):\n",
        "  x['Type'] = x['Type'].map({1:1, 2:2, 0:4, 3:4}) \n",
        "  return x"
      ],
      "metadata": {
        "id": "ArdA0JyVITJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformación de los conjuntos\n",
        "\n",
        "#Pipeline para Variables Numéricas\n",
        "numericalImp_pipeline = Pipeline(steps=[('numImp', SimpleImputer(strategy='median'))])\n",
        "numImp_pipe_names = ['Page total likes', 'Post Month', 'Post Weekday', 'Post Hour']\n",
        "\n",
        "#Pipeline para Variables Numéricas\n",
        "numericalPow_pipeline = Pipeline(steps=[('numPow', FunctionTransformer(mi_power))])\n",
        "numPow_pipe_names = ['Page total likes']\n",
        "\n",
        "#Pipeline para Variables Categóricas\n",
        "categoricalImp_pipeline = Pipeline(steps=[('catImp', SimpleImputer(strategy='most_frequent'))])\n",
        "catImp_pipe_names = ['Category', 'Paid', 'Type']\n",
        "\n",
        "#Pipeline para Variables Categóricas\n",
        "categoricalMap_pipeline = Pipeline(steps=[('catMap', FunctionTransformer(mi_type))])\n",
        "catMap_pipe_names = ['Type']\n",
        "\n",
        "#Pipeline para Variables Categóricas\n",
        "categoricalOhE_pipeline = Pipeline(steps=[('catOhE', OneHotEncoder(drop='first', handle_unknown='ignore'))])\n",
        "catOhE_pipe_names = ['Category', 'Paid', 'Type']\n",
        "\n",
        "# ColumnTransformer nos permite aplicar diferentes preprocesamientos a distintas\n",
        "# secciones de los datos\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "    ('numImp', numericalImp_pipeline, numImp_pipe_names),\n",
        "    ('numPow', numericalPow_pipeline, numPow_pipe_names),\n",
        "    ('catImp', categoricalImp_pipeline, catImp_pipe_names),\n",
        "    ('catMap', categoricalMap_pipeline, catMap_pipe_names),\n",
        "    ('catOhE', categoricalOhE_pipeline, catOhE_pipe_names)\n",
        "],\n",
        "remainder='passthrough')"
      ],
      "metadata": {
        "id": "7KXnlNzAC69Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Representación gráfica de los pipeline en ColumnTransformer\n",
        "preprocessor"
      ],
      "metadata": {
        "id": "vslQ8jLL7fuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-5.**"
      ],
      "metadata": {
        "id": "Rv7KFq-mC7PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Partición de los datos en 100 para validación y 300 para entrenamiento\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_v, y_train_v, test_size=100, shuffle=True)\n",
        "\n",
        "print('\\033[1m' + 'Forma de X_train:' + '\\033[0m', X_train.shape)\n",
        "print('\\033[1m' + 'Forma de y_train:' + '\\033[0m', y_train.shape)\n",
        "print('\\033[1m' + 'Forma de X_val:' + '\\033[0m', X_val.shape)"
      ],
      "metadata": {
        "id": "eTU7XEITXT_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejecución del modelo DummyRegressor\n",
        "\n",
        "#Fit de los datos de entrenamiento para el modelo DummyRegressor\n",
        "Xtrain_ct=preprocessor.fit_transform(X_train)\n",
        "\n",
        "#Transformación de los conjuntos de entrenamiento y validación para la variable de salida\n",
        "ytrain_Log= mi_log(y_train['LPE'])\n",
        "yval_Log= mi_log(y_val['LPE'])\n",
        "\n",
        "#Preparación del modelo DummyRegressor con la estrategia 'mean'\n",
        "model_DR= DummyRegressor(strategy='mean')\n",
        "model_DR.fit(Xtrain_ct, ytrain_Log)\n",
        "\n",
        "#Transformación de los datos de validación para las predicciones del modelo\n",
        "Xval_ct=preprocessor.transform(X_val)\n",
        "\n",
        "#Obtención de las predicciones del modelo usando los datos de entrenamiento y validación ajustados al DummyRegressor\n",
        "yhat_DR_t= model_DR.predict(Xtrain_ct) \n",
        "yhat_DR_v= model_DR.predict(Xval_ct)\n",
        "\n",
        "#Cálculo de los errores RMSE, MAE y MAPE en los conjuntos de entrenamiento y validación\n",
        "rmse_train = mi_RMSE(ytrain_Log, yhat_DR_t)\n",
        "rmse_val = mi_RMSE(yval_Log, yhat_DR_v)\n",
        "\n",
        "mae_train = mi_MAE(ytrain_Log, yhat_DR_t)\n",
        "mae_val = mi_MAE(yval_Log, yhat_DR_v)\n",
        "\n",
        "mape_train = mi_MAPE(ytrain_Log, yhat_DR_t)\n",
        "mape_val = mi_MAPE(yval_Log, yhat_DR_v)\n",
        "\n",
        "# Imprimir los errores\n",
        "print('\\033[1m' + \"RMSE en entrenamiento:\" + '\\033[0m', rmse_train)\n",
        "print('\\033[1m' + \"RMSE en validación:\" + '\\033[0m', rmse_val)\n",
        "print('\\033[1m' + \"MAE en entrenamiento:\" + '\\033[0m', mae_train)\n",
        "print('\\033[1m' + \"MAE en validación:\" + '\\033[0m', mae_val)\n",
        "print('\\033[1m' + \"MAPE en entrenamiento:\" + '\\033[0m', mape_train)\n",
        "print('\\033[1m' + \"MAPE en validación:\" + '\\033[0m', mape_val)"
      ],
      "metadata": {
        "id": "Jjq9cGowXvHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-6.**"
      ],
      "metadata": {
        "id": "W2S7LI0NC9wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición de la función con las listas de los modelos a ejecutar\n",
        "\n",
        "def get_models():\n",
        "\n",
        "  #Definición inicial de las variables 'models' y 'model_names'\n",
        "  models= list()\n",
        "  model_names= list()\n",
        "\n",
        "  #Modelo LR - Regresión Lineal Múltiple\n",
        "  models.append(LinearRegression())\n",
        "  model_names.append('LR')\n",
        "\n",
        "  #Modelo RF - Bosque Aleatorio\n",
        "  models.append(RandomForestRegressor())\n",
        "  model_names.append('RF')\n",
        "\n",
        "  #Modelo MLP - Red Neuronal Artificial / Perceptrón Lineal Multicapa\n",
        "  models.append(MLPRegressor(max_iter=10000, hidden_layer_sizes=(15,), activation= 'tanh'))\n",
        "  model_names.append('MLP')\n",
        "  \n",
        "  return models, model_names"
      ],
      "metadata": {
        "id": "x6uBleJUC_AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluación de los modelos LR, RF y MLP con el conjunto de datos de entrenamiento y validación\n",
        "\n",
        "#Importado de librería 'warnings' para eliminar el desplegado de los errores\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Carga de los modelos a comparar\n",
        "models, model_names = get_models()\n",
        "\n",
        "#Definición de la variable 'resultados'\n",
        "resultados=list()\n",
        "\n",
        "for i in range(len(models)):\n",
        "\n",
        "  # Uso del preprocesador definido anteriormente con el modelo correspondiente para cada iteracion\n",
        "  pipe= Pipeline(steps= [('ColT', preprocessor), ('m', models[i])])\n",
        "  \n",
        "  #Definición del tipo de validacion cruzada a utilizar\n",
        "  kfold= RepeatedKFold(n_splits=5, n_repeats=3)\n",
        "\n",
        "  #Definición de las métricas a utilizar\n",
        "  mis_metricas= {'rmse':make_scorer(mi_RMSE), 'mae':make_scorer(mi_MAE), 'mape':make_scorer(mi_MAPE)}\n",
        "\n",
        "  #Resultados obtenidos por validación cruzada\n",
        "  scores= cross_validate(pipe,\n",
        "                         X_train_v, mi_log(np.ravel(y_train_v)),\n",
        "                         scoring= mis_metricas,\n",
        "                         cv= kfold,\n",
        "                         return_train_score=True,\n",
        "                         n_jobs=-1,\n",
        "                         error_score='raise')\n",
        "  \n",
        "  resultados.append(scores)\n",
        "\n",
        "  print('%s - Train\\nmean RMSE: %.4f \\nmean MAE: %.3f \\nmean MAPE: %.4f \\n' % (model_names[i],\n",
        "                                                                               np.mean(scores['train_rmse']),\n",
        "                                                                               np.mean(scores['train_mae']),\n",
        "                                                                               np.mean(scores['train_mape'])\n",
        "                                                                               ))\n",
        "  \n",
        "  print('%s - Validation\\nmean RMSE: %.4f \\nmean MAE: %.3f \\nmean MAPE: %.4f \\n' % (model_names[i],\n",
        "                                                                               np.mean(scores['test_rmse']),\n",
        "                                                                               np.mean(scores['test_mae']),\n",
        "                                                                               np.mean(scores['test_mape'])\n",
        "                                                                               ))\n"
      ],
      "metadata": {
        "id": "n0P_AcyjC_Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones**\n",
        "\n",
        "Con base a las respuestas obtenidas en esta parte, se pueden observar valores de métricas menores a las obtenidas por los autores del artículo de investigación."
      ],
      "metadata": {
        "id": "vm8wp5uAUHG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-7.**"
      ],
      "metadata": {
        "id": "iCNGx4TQ8CFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Valores en diagrama de caja y bigotes con la métrica \"MAPE\"\n",
        "sns.set(rc={'figure.figsize':(8,4)})\n",
        "\n",
        "bpMAPE_1 = list()\n",
        "for i in range(len(resultados)):\n",
        "  rr = resultados[i]['test_mape']\n",
        "  bpMAPE_1.append(rr)\n",
        "\n",
        "plt.boxplot(bpMAPE_1, labels=model_names, showmeans=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ewvwUcJX78y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones**\n",
        "\n",
        "Con base a los diagramas de caja y bigote anteriormente mostrados, en comparación con los resultados obtenidos por los autores del artículo,"
      ],
      "metadata": {
        "id": "DZ8OkmuTUkJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-8.**"
      ],
      "metadata": {
        "id": "tzQn5NR78GFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GridSearch para la búsqueda de los mejores hiperparámetros para el MLP\n",
        "\n",
        "#Definición del modelo del perceptrón lineal multicapa\n",
        "m_MLP= MLPRegressor(max_iter=20000)\n",
        "\n",
        "#Definición del pipeline a utilizar\n",
        "pipe2= Pipeline(steps=[('ColT', preprocessor),('mlp', m_MLP)])\n",
        "\n",
        "#Definición del tipo de validacion cruzada a utilizar\n",
        "kfold2= RepeatedKFold(n_splits=5, n_repeats=3)\n",
        "\n",
        "#Definición de los hiperparámetros a iterar en el grid\n",
        "mlp_grid_d= dict(\n",
        "    mlp__hidden_layer_sizes=[(10,), (7,7), (15,), (7,8), (3,8)],\n",
        "    mlp__alpha= [0.001 ,0.01, 1.0, 10.0],\n",
        "    mlp__learning_rate= ['constant', 'invscaling', 'adaptive'],\n",
        "    mlp__learning_rate_init=[0.01, 0.1, 1.0],\n",
        "    mlp__activation= ['logistic', 'tanh']\n",
        ")\n",
        "\n",
        "#Ejecución del grid search para obtener los mejores hiperparámetros\n",
        "mlp_grid = GridSearchCV(estimator=pipe2,\n",
        "                        param_grid=mlp_grid_d,\n",
        "                        cv=kfold2,\n",
        "                        scoring= make_scorer(mi_MAPE),\n",
        "                        n_jobs=-1,\n",
        "                        error_score='raise')\n",
        "\n",
        "#Grid Fit con los datos del conjunto de entrenamiento original\n",
        "mlp_grid.fit(X_train_v, mi_log(y_train_v['LPE']))\n",
        "\n",
        "#Obtención de los mejores parámetros y la mejor respuesta\n",
        "print('\\033[1m' + \"Mejores parámetros en la búsqueda de cuadrilla (GridSearchCV):\" + '\\033[0m', mlp_grid.best_params_)\n",
        "print('\\033[1m' + \"Mejor puntuación de la búsqueda de cuadrilla (GridSearchCV):\" + '\\033[0m', mlp_grid.best_score_)\n",
        "print('\\033[1m' + 'Métrica utilizada:'+ '\\033[0m', mlp_grid.scoring)"
      ],
      "metadata": {
        "id": "EBKcp0278IQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones**\n",
        "\n",
        "Con base a los diagramas de caja y bigote anteriormente mostrados, en comparación con los resultados obtenidos por los autores del artículo,"
      ],
      "metadata": {
        "id": "AQyNNRlrQJvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-9.**"
      ],
      "metadata": {
        "id": "mASNrZWs8JTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Comprobación de Hiperparámetros en el modelo\n",
        "\n",
        "#Mejores Hiperparámetros correspondientes \n",
        "model_MLP= MLPRegressor(hidden_layer_sizes=(15,),\n",
        "                        activation= 'logistic',\n",
        "                        max_iter=20000,\n",
        "                        alpha=10.0,\n",
        "                        learning_rate='invscaling',\n",
        "                        learning_rate_init=1.0)\n",
        " \n",
        "#Fit_transform del conjunto de entrenamiento inicial a través del pipeline\n",
        "Xtvct= preprocessor.fit_transform(X_train_v)\n",
        "X_testct= preprocessor.transform(X_test)\n",
        "\n",
        "#Entrenamiento del modelo MLP\n",
        "model_MLP.fit(Xtvct, mi_log(y_train_v['LPE']))\n",
        "\n",
        "#Obtención de predicciones del modelo\n",
        "yhat= model_MLP.predict(X_testct)\n",
        "\n",
        "#Métricas Obtenidas con las predicciones\n",
        "print('\\033[1m' + \"MLP - RMSE: %.4f\"  % mi_RMSE(np.ravel(mi_log(y_test)),yhat) + '\\033[0m')\n",
        "print('\\033[1m' + \"MLP - MAE: %.4f\"  % mi_MAE(np.ravel(mi_log(y_test)),yhat) + '\\033[0m')\n",
        "print('\\033[1m' + \"MLP - MAPE: %.2f %%\"  % mi_MAPE(np.ravel(mi_log(y_test)),yhat) + '\\033[0m')"
      ],
      "metadata": {
        "id": "L96ZINI3Avug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición del modelo con los mejores parámetros\n",
        "mMLPbest= MLPRegressor(hidden_layer_sizes=(15,),\n",
        "                        activation= 'logistic',\n",
        "                        max_iter=20000,\n",
        "                        alpha=10.0,\n",
        "                        learning_rate='invscaling',\n",
        "                        learning_rate_init=1.0)\n",
        "\n",
        "#Fit de datos de prueba en modelo con los mejores parámetros\n",
        "mMLPbest.fit(X_test, mi_log(y_test['LPE']))\n",
        "\n",
        "#Definición de la variable 'importance' para observar las características más importantes\n",
        "importance= permutation_importance(mMLPbest, X_test, mi_log(y_test['LPE']), n_repeats=10)\n",
        "\n",
        "for i,v in enumerate(importance['importances_mean']):\n",
        "  print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "\n",
        "#Generación de diagrama de barras con las características más importantes\n",
        "fig, axs= plt.subplots()\n",
        "plt.barh([x for x in range(len(importance['importances_mean']))], importance['importances_mean'])\n",
        "plt.yticks(range(X_test.shape[1]), np.array(X_test.columns))\n",
        "plt.show"
      ],
      "metadata": {
        "id": "X6HJP9hb8LCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones**\n",
        "\n",
        "Con base a los resultados obtenidos, el valor de alpha obtenido por el gridsearch no fue el que nos entregó la mejor información posible. Al contrario, aumentó el valor de las métricas, por lo que se decidió disminuirlo para comprobar el efecto."
      ],
      "metadata": {
        "id": "7DHFJxS3DT5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-10.**"
      ],
      "metadata": {
        "id": "VUIcDshs8MzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GridSearch para la búsqueda de los mejores hiperparámetros para el RF\n",
        "\n",
        "#Definición del modelo de bosque aleatorio\n",
        "m_RF= RandomForestRegressor()\n",
        "\n",
        "#Definición del pipeline a utilizar\n",
        "pipe3= Pipeline(steps=[('ColT', preprocessor),('rf', m_RF)])\n",
        "\n",
        "#Definición del tipo de validacion cruzada a utilizar\n",
        "kfold3= RepeatedKFold(n_splits=5, n_repeats=3)\n",
        "\n",
        "#Definición de los hiperparámetros a iterar en el grid\n",
        "rf_grid_d= [\n",
        "    {\n",
        "     'rf__max_depth':[2,3,4,5,6],\n",
        "     'rf__min_samples_split': [2,3,4,5,6,7],\n",
        "     'rf__ccp_alpha' : [0.001, 0.01, 0.1]\n",
        "    }\n",
        "]\n",
        "\n",
        "#Ejecución del grid search para obtener los mejores hiperparámetros\n",
        "rf_grid = GridSearchCV(estimator=pipe3,\n",
        "                        param_grid=rf_grid_d,\n",
        "                        cv=kfold3,\n",
        "                        scoring= make_scorer(mi_MAPE),\n",
        "                        n_jobs=-1,\n",
        "                        error_score='raise')\n",
        "\n",
        "#Grid Fit con los datos del conjunto de entrenamiento original\n",
        "rf_grid.fit(X_train_v, mi_log(y_train_v['LPE']))\n",
        "\n",
        "#Obtención de los mejores parámetros y la mejor respuesta\n",
        "print('\\033[1m' + \"Mejores parámetros en la búsqueda de cuadrilla (GridSearchCV):\" + '\\033[0m', rf_grid.best_params_)\n",
        "print('\\033[1m' + \"Mejor puntuación de la búsqueda de cuadrilla (GridSearchCV):\" + '\\033[0m', rf_grid.best_score_)\n",
        "print('\\033[1m' + 'Métrica utilizada:'+ '\\033[0m', rf_grid.scoring)"
      ],
      "metadata": {
        "id": "0lKNJNIt8N88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comprobación de Hiperparámetros en el modelo\n",
        "\n",
        "#Mejores Hiperparámetros correspondientes \n",
        "model_RF= RandomForestRegressor(ccp_alpha=0.1,\n",
        "                                max_depth= 6,\n",
        "                                min_samples_split=6,\n",
        ")\n",
        "\n",
        "#Fit_transform del conjunto de entrenamiento inicial a través del pipeline\n",
        "Xtvct= preprocessor.fit_transform(X_train_v)\n",
        "X_testct= preprocessor.transform(X_test)\n",
        "\n",
        "#Entrenamiento del modelo RF\n",
        "model_RF.fit(Xtvct, mi_log(y_train_v))\n",
        "\n",
        "#Obtención de predicciones del modelo\n",
        "yhat=model_RF.predict(X_testct)\n",
        "\n",
        "#Métricas Obtenidas con las predicciones\n",
        "print('\\033[1m' + \"RF - RMSE: %.4f\"  % mi_RMSE(np.ravel(mi_log(y_test)),yhat) + '\\033[0m')\n",
        "print('\\033[1m' + \"RF - MAE: %.4f\"  % mi_MAE(np.ravel(mi_log(y_test)),yhat) + '\\033[0m')\n",
        "print('\\033[1m' + \"RF - MAPE: %.2f %%\"  % mi_MAPE(np.ravel(mi_log(y_test)),yhat) + '\\033[0m')"
      ],
      "metadata": {
        "id": "Inkq5YQe8PED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición del modelo con los mejores parámetros\n",
        "mRFbest= RandomForestRegressor(ccp_alpha=0.1, max_depth= 5, min_samples_split=5,)\n",
        "\n",
        "#Fit de datos de prueba en modelo con los mejores parámetros\n",
        "mRFbest.fit(X_test, mi_log(y_test['LPE']))\n",
        "\n",
        "#Definición de la variable 'importance' para observar las características más importantes\n",
        "importance= permutation_importance(mRFbest, X_test, mi_log(y_test['LPE']), n_repeats=10)\n",
        "\n",
        "for i,v in enumerate(importance['importances_mean']):\n",
        "  print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "\n",
        "#Generación de diagrama de barras con las características más importantes\n",
        "fig, axs= plt.subplots()\n",
        "plt.barh([x for x in range(len(importance['importances_mean']))], importance['importances_mean'])\n",
        "plt.yticks(range(X_test.shape[1]), np.array(X_test.columns))\n",
        "plt.show"
      ],
      "metadata": {
        "id": "kGydWRYxDlHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones**\n",
        "\n",
        "Con base a los resultados obtenidos , "
      ],
      "metadata": {
        "id": "rjG9MB1NQOTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-11.**"
      ],
      "metadata": {
        "id": "5LJl6oql8Pc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GridSearch para la búsqueda de los mejores hiperparámetros para el LR\n",
        "\n",
        "#Definición del modelo de regresión lineal múltiple\n",
        "m_LR= LinearRegression()\n",
        "\n",
        "#Definición del pipeline a utilizar\n",
        "pipe4= Pipeline(steps=[('ColT', preprocessor),('lr', m_LR)])\n",
        "\n",
        "#Definición del tipo de validacion cruzada a utilizar\n",
        "kfold4= RepeatedKFold(n_splits=5, n_repeats=3)\n",
        "\n",
        "#Definición de los hiperparámetros a iterar en el grid\n",
        "lr_grid_d= [\n",
        "    {\n",
        "     'lr__fit_intercept': [True, False],\n",
        "     'lr__copy_X': [True, False],\n",
        "     'lr__positive': [True, False]\n",
        "    }\n",
        "]\n",
        "\n",
        "#Ejecución del grid search para obtener los mejores hiperparámetros\n",
        "lr_grid = GridSearchCV(estimator=pipe4,\n",
        "                        param_grid=lr_grid_d,\n",
        "                        cv=kfold4,\n",
        "                        scoring= make_scorer(mi_MAPE),\n",
        "                        n_jobs=-1,\n",
        "                        error_score='raise')\n",
        "\n",
        "#Grid Fit con los datos del conjunto de entrenamiento original\n",
        "lr_grid.fit(X_train_v, mi_log(y_train_v['LPE']))\n",
        "\n",
        "#Obtención de los mejores parámetros y la mejor respuesta\n",
        "print('\\033[1m' + \"Mejores parámetros en la búsqueda de cuadrilla (GridSearchCV):\" + '\\033[0m', lr_grid.best_params_)\n",
        "print('\\033[1m' + \"Mejor puntuación de la búsqueda de cuadrilla (GridSearchCV):\" + '\\033[0m', lr_grid.best_score_)\n",
        "print('\\033[1m' + 'Métrica utilizada:'+ '\\033[0m', lr_grid.scoring)"
      ],
      "metadata": {
        "id": "-YiSnt9t8RfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comprobación de Hiperparámetros en el modelo\n",
        "\n",
        "#Hiperparámetros correspondientes \n",
        "model_LR= LinearRegression(copy_X=True,\n",
        "                           fit_intercept=False,\n",
        "                           positive=True    \n",
        ")\n",
        "\n",
        "#Fit_transform del conjunto de entrenamiento inicial a través del pipeline\n",
        "Xtvct= preprocessor.fit_transform(X_train_v)\n",
        "X_testct= preprocessor.transform(X_test)\n",
        "\n",
        "#Entrenamiento del modelo LR\n",
        "model_LR.fit(Xtvct, mi_log(y_train_v))\n",
        "\n",
        "#Obtención de predicciones del modelo\n",
        "yhat=model_LR.predict(X_testct)\n",
        "\n",
        "#Métricas Obtenidas con las predicciones\n",
        "print('\\033[1m' + \"LR - RMSE: %.4f\"  % mi_RMSE((mi_log(y_test)),yhat) + '\\033[0m')\n",
        "print('\\033[1m' + \"LR - MAE: %.4f\"  % mi_MAE((mi_log(y_test)),yhat) + '\\033[0m')\n",
        "print('\\033[1m' + \"LR - MAPE: %.2f %%\"  % mi_MAPE((mi_log(y_test)),yhat) + '\\033[0m')"
      ],
      "metadata": {
        "id": "2iRA78ZC8Rbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición del modelo con los mejores parámetros\n",
        "mLRbest= LinearRegression()\n",
        "\n",
        "#Fit de datos de prueba en modelo con los mejores parámetros\n",
        "mLRbest.fit(X_test, mi_log(y_test['LPE']))\n",
        "\n",
        "#Definición de la variable 'importance' para observar las características más importantes\n",
        "importance= permutation_importance(mLRbest, X_test, mi_log(y_test['LPE']), n_repeats=10)\n",
        "\n",
        "for i,v in enumerate(importance['importances_mean']):\n",
        "  print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "\n",
        "#Generación de diagrama de barras con las características más importantes\n",
        "fig, axs= plt.subplots()\n",
        "plt.barh([x for x in range(len(importance['importances_mean']))], importance['importances_mean'])\n",
        "plt.yticks(range(X_test.shape[1]), np.array(X_test.columns))\n",
        "plt.show"
      ],
      "metadata": {
        "id": "yPBiuxcpFbnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones**\n",
        "\n",
        "Con base a los resultados obtenidos , "
      ],
      "metadata": {
        "id": "DJwJO_lvQP8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-12.**"
      ],
      "metadata": {
        "id": "IKW72uyk8Sbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones**\n",
        "\n",
        "Con base a los resultados obtenidos en esta actividad en comparación con los obtenidos por los autores del artículo de Moro-Rita-Vala, "
      ],
      "metadata": {
        "id": "e7c66u1fURJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Fin de la Actividad de la semana 7.**"
      ],
      "metadata": {
        "id": "7ql_r2G-DB_m"
      }
    }
  ]
}