{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GNByOTGFntOtfHmtiIdQkSlkZ7GFbuAj",
      "authorship_tag": "ABX9TyO2K/XAjBYST0AXw+uGKVBv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigoromanguzman/Actividades_Aprendizaje-/blob/main/language_model_rnn_from_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bHMJ7LQ3WOn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Utility libraries\n",
        "import random\n",
        "import re\n",
        "\n",
        "\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcruplZaz2cs",
        "outputId": "342cb491-2835-4b59-f22f-757b39afd031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_s_path = \"/content/drive/MyDrive/newsSpace\""
      ],
      "metadata": {
        "id": "5USCIixw0paW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function for detecting urls\n",
        "def is_url(s):\n",
        "    # A simple regex to check for a basic URL structure\n",
        "    return re.match(r'https?://', s) is not None\n"
      ],
      "metadata": {
        "id": "HZWg_1WuDAfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will separate the batches as the elements from an individual article\n",
        "data_articles = []\n",
        "# Lets collect all the unique ocurrences of each word\n",
        "# to create the dictionary\n",
        "vocabulary = set([])\n",
        "num_articles = 5\n",
        "counter = 0\n",
        "with open(news_s_path, encoding='ISO-8859-1') as f:\n",
        "    while (counter<num_articles):\n",
        "      line = f.readline()\n",
        "      if not line:\n",
        "          break\n",
        "      data_line = line.strip().split()\n",
        "      url_index = next((i for i, item in enumerate(data_line) if is_url(item)), None)\n",
        "      # We will get only the articles which have a url\n",
        "      if url_index is not None:\n",
        "        # We take after the url\n",
        "        article = line.strip().split()[url_index+1::]\n",
        "        [vocabulary.add(i) for i in article]\n",
        "        data_articles.append(article)\n",
        "      counter += 1\n",
        "print(\"Vocabulary size -> \", len(vocabulary))\n",
        "# print(data_articles[:100])\n",
        "# print(list(vocabulary)[:200])\n"
      ],
      "metadata": {
        "id": "xt4wscHo214n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215d18b5-8353-4c6a-8bfe-a8cf6d7f104c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size ->  22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data_articles, vocabulary):\n",
        "    # Lowercase and tokenize the text\n",
        "    processed_articles = []\n",
        "    for article in data_articles:\n",
        "        processed_article = [word.lower() for word in article if word.isalpha()]\n",
        "        processed_articles.append(processed_article)\n",
        "        vocabulary.update(processed_article)\n",
        "    return processed_articles, vocabulary\n",
        "\n",
        "data_articles, vocabulary = preprocess_data(data_articles, vocabulary)"
      ],
      "metadata": {
        "id": "D6lsEiAJf1RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and test\n",
        "test_percentage = 0.2\n",
        "random.shuffle(data_articles)\n",
        "split_point = int(len(data_articles)*test_percentage)\n",
        "training_set = data_articles[split_point::]\n",
        "test_set = data_articles[:split_point:]"
      ],
      "metadata": {
        "id": "tSQMCFvbDjJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embedding initialization with random values\n",
        "mean = 0\n",
        "# std_dev = 0.001\n",
        "vocab_size = len(vocabulary)\n",
        "embedding_size = 220 #Hardcoded value for the size of the vectors\n",
        "word_embeddings = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_size))\n",
        "word_embeddings = word_embeddings / embedding_size  # Scaling by embedding size\n",
        "hidden_size = 500  # Size of the hidden state vectors\n",
        "\n",
        "# Create a dictionary to map words to their embedding vectors\n",
        "word_to_embedding = {}\n",
        "for i, word in enumerate(vocabulary):\n",
        "    word_to_embedding[word] = word_embeddings[i]"
      ],
      "metadata": {
        "id": "enODZ5lLI3qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights randomly\n",
        "# W_e = np.random.normal(mean, std_dev, size=(hidden_size, embedding_size))\n",
        "# W_h = np.random.normal(mean, std_dev, size=(hidden_size,hidden_size))\n",
        "# W_y = np.random.normal(mean, std_dev, size=(hidden_size,vocab_size))# Hidden to output\n",
        "# bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "# by = np.zeros((vocab_size, 1))  # Output bias\n",
        "# def initialize_parameters(vocab_size, embedding_size, hidden_size):\n",
        "#     # Xavier initialization for weights\n",
        "W_e = np.random.randn(hidden_size, embedding_size) * np.sqrt(2.0 / (hidden_size + embedding_size))\n",
        "W_h = np.random.randn(hidden_size, hidden_size) * np.sqrt(2.0 / (hidden_size + hidden_size))\n",
        "W_y = np.random.randn(hidden_size, vocab_size) * np.sqrt(2.0 / (vocab_size + hidden_size))\n",
        "\n",
        "# Biases initialized to zero\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((vocab_size, 1))\n",
        "\n",
        "#     return W_e, W_h, W_y, bh, by"
      ],
      "metadata": {
        "id": "l0yWqAXzbNHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoded vocabulary\n",
        "vocabulary_list = list(vocabulary)\n",
        "\n",
        "vocabulary_list.sort()  # Sorting to ensure consistent indexing\n",
        "\n",
        "# Create a dictionary that maps words to indices based on the sorted order\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "word_to_index['<UNK>'] = len(vocabulary)  # Add '<UNK>' token with a new index"
      ],
      "metadata": {
        "id": "6A0YL2tBVh3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_indices(sentence, word_to_index):\n",
        "    return [word_to_index.get(word, word_to_index['<UNK>']) for word in sentence.split()]"
      ],
      "metadata": {
        "id": "X5sf0WJdmKdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return np.exp(x)/sum(np.exp(x))"
      ],
      "metadata": {
        "id": "4L30yXQBIAkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tahnh(x):\n",
        "  return np.tanh(x)"
      ],
      "metadata": {
        "id": "U0UwotcsMbGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input the probabilies and the true value\n",
        "def cross_entropy(y_probs, y_true):\n",
        "    # y_probs is a dictionary with timestep as key and array of probabilities as value\n",
        "    # y_true is a list or array of true class indices\n",
        "    total_loss = 0\n",
        "    for t, true_index in enumerate(y_true):\n",
        "        if t in y_probs:\n",
        "            # Select the probability corresponding to the true class at timestep t\n",
        "            prob_true_class = y_probs[t][true_index, 0]  # Adjust indexing if necessary\n",
        "            # Calculate the negative log of this probability\n",
        "            total_loss += -np.log(prob_true_class)\n",
        "    # Average the loss over all timesteps\n",
        "    average_loss = total_loss / len(y_true)\n",
        "    return average_loss\n"
      ],
      "metadata": {
        "id": "NKUkOR4mM-Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_dropout(X, dropout_rate):\n",
        "    mask = np.random.binomial(1, 1 - dropout_rate, size=X.shape)\n",
        "    return np.multiply(X, mask)"
      ],
      "metadata": {
        "id": "8Lw3pKrsQCtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input the sentnece as thie index representation\n",
        "# parameters: dictionary with keys -> W_e, W_h, W_y, bh, by\n",
        "def forward_pass(indices, word_embeddings,parameters):\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Initial hidden state\n",
        "\n",
        "    states = {'es': {},'hs': {},'ys': {},'ps': {}}\n",
        "    states['hs'][-1] = np.copy(h_prev)\n",
        "\n",
        "\n",
        "    # Forward pass\n",
        "    for t in range(len(indices)):\n",
        "        states['es'][t] = word_embeddings[indices[t]].reshape(-1, 1)  # Embedding vector for current input word\n",
        "        # es_t_reshaped = states['es'][t].reshape(-1, 1)  # Reshape to (150, 1)\n",
        "\n",
        "        # print(\"parameters['W_h']\")\n",
        "        # print(parameters['W_h'].shape)\n",
        "        # print(\"h_prev\")\n",
        "        # print(h_prev.shape)\n",
        "        # print(\"w_e\")\n",
        "        # print(parameters['W_e'].shape)\n",
        "        # print(\"es_t_reshaped\")\n",
        "        # print(es_t_reshaped.shape)\n",
        "        # print(\"parameters['bh']\")\n",
        "        # print(parameters['bh'].shape)\n",
        "\n",
        "        h_rec = np.dot(parameters['W_h'], h_prev) + np.dot(parameters['W_e'], states['es'][t]) + parameters['bh']\n",
        "        # print(\"HREC\")\n",
        "        # print(h_rec.shape)\n",
        "        states['hs'][t] = tahnh(h_rec)  # Hidden state\n",
        "\n",
        "        states['ys'][t] = np.dot(parameters['W_y'].T, states['hs'][t]) + parameters['by']  # Unnormalized log probabilities for next words\n",
        "        states['ps'][t] = softmax(states['ys'][t])  # Probabilities for next words\n",
        "        h_prev = states['hs'][t]  # Pass the current hidden state to the next time step\n",
        "        # states['hs'][t] = apply_dropout(states['hs'][t], 0.2)\n",
        "    # print(\"Hidden state:\", states['hs'][t])\n",
        "    # print(\"Predictions:\", states['ps'][t])\n",
        "    return states"
      ],
      "metadata": {
        "id": "7KgFU-K4HSrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "# outputs -> dictionary of dictionaries: es, hs, ys, ps\n",
        "# parameters -> dictionary with: W_h, W_y, bh, by\n",
        "\n",
        "def backward_pass(indices,outputs, parameters):\n",
        "    gradients = {\n",
        "        'dW_e': np.zeros_like(W_e),\n",
        "        'dW_h': np.zeros_like(parameters['W_h']),\n",
        "        'dW_y': np.zeros_like(parameters['W_y']),\n",
        "        'dbh': np.zeros_like(parameters['bh']),\n",
        "        'dby': np.zeros_like(parameters['by'])\n",
        "    }\n",
        "\n",
        "    dh_next = np.zeros_like(outputs['hs'][0])\n",
        "    for t in reversed(range(len(indices)-1)):\n",
        "        dy = np.copy(outputs['ps'][t])  # Copy softmax probabilities\n",
        "\n",
        "        true_label_index = indices[t]\n",
        "        dy[true_label_index] -= 1\n",
        "\n",
        "        temp_product = np.dot(dy, outputs['hs'][t].T)  # Results in shape (1448, 500)\n",
        "        gradients['dW_y'] += temp_product.T\n",
        "\n",
        "        gradients['dby'] += dy\n",
        "        dh = np.dot(parameters['W_y'], dy) + dh_next  # Backprop into h\n",
        "        dh_rec = (1 - outputs['hs'][t] * outputs['hs'][t]) * dh  # Backprop through tanh nonlinearity\n",
        "        gradients['dbh'] += dh_rec\n",
        "\n",
        "        gradients['dW_h'] += np.dot(dh_rec, outputs['hs'][t-1].T)\n",
        "        gradients['dW_e'] += np.dot(dh_rec, outputs['es'][t].T)\n",
        "        dh_next = np.dot(parameters['W_h'].T, dh_rec)\n",
        "\n",
        "    for gradientKey in gradients:\n",
        "        np.clip(gradients[gradientKey], -5, 5, out=gradients[gradientKey])  # Clip to mitigate exploding gradients\n",
        "    return gradients\n"
      ],
      "metadata": {
        "id": "h_c6LcuMQEEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update weights\n",
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    # parameters and gradients are dictionaries with the same keys: 'W_e', 'W_h', 'W_y', 'bh', 'by'\n",
        "    for key in parameters.keys():\n",
        "        parameters[key] -= learning_rate * gradients['d' + key]"
      ],
      "metadata": {
        "id": "OXxWrosQALZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# And we have our parameters 'W_e', 'W_h', 'W_y', 'bh', 'by'\n",
        "parameters = {\n",
        "    'W_e': W_e,\n",
        "    'W_h': W_h,\n",
        "    'W_y': W_y,\n",
        "    'bh': bh,\n",
        "    'by': by\n",
        "}\n",
        "def initialize_gradients(parameters):\n",
        "    gradients = {}\n",
        "    for key in parameters.keys():\n",
        "        gradients['d' + key] = np.zeros_like(parameters[key])\n",
        "    return gradients\n",
        "\n",
        "# Usage\n",
        "gradients = initialize_gradients(parameters)\n",
        "learning_rate = 0.01 #0.005"
      ],
      "metadata": {
        "id": "WiiTahqQ68kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def training(mini_batches,gradients,parameters,vocabulary,learning_rate,epochs):\n",
        "# def training(data_articles, word_to_index, word_embeddings, parameters, vocabulary_list, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for batch_idx, sentence in enumerate(data_articles, start=0):\n",
        "            # print(\"Sentence\")\n",
        "            # print(sentence)\n",
        "            indices = [word_to_index.get(word, word_to_index['<UNK>']) for word in sentence]  # Tokenize the sentence\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = forward_pass(indices, word_embeddings, parameters)\n",
        "\n",
        "            # Create true labels (next words) for this batch\n",
        "            true_labels = indices[1:]  # Assuming the next word is the true label for each word in the sentence\n",
        "\n",
        "            # Backward pass\n",
        "            gradients = backward_pass(indices, outputs, parameters)\n",
        "\n",
        "            # Update parameters\n",
        "            update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "            # Calculate and print loss\n",
        "            loss = cross_entropy(outputs['ps'], true_labels)\n",
        "            total_loss += loss\n",
        "\n",
        "\n",
        "            # Calculate accuracy for this batch\n",
        "            predicted_words = [vocabulary_list[np.argmax(outputs['ps'][output])] for output in outputs['ps']]\n",
        "            # print(\"Predicted words\")\n",
        "            # print(predicted_words)\n",
        "            predicted_indices = [word_to_index.get(word, word_to_index['<UNK>']) for word in predicted_words]\n",
        "            correct_predictions += sum([1 for predicted, true in zip(predicted_indices[:-1], true_labels) if predicted == true])\n",
        "            total_predictions += len(true_labels)\n",
        "\n",
        "            # Print loss and accuracy at specified intervals\n",
        "\n",
        "        # Print average loss and final training accuracy for the epoch\n",
        "        if epoch % 5 == 0:  # Print some debug info every 10 epochs\n",
        "          print(\"PREDICTED WORDS\")\n",
        "          print(predicted_words[:-1])\n",
        "          print(\"Target words\")\n",
        "          print(sentence[1:])\n",
        "          # print(\"How the probs look like\")\n",
        "          # print(outputs['ps'][0])\n",
        "        avg_loss = total_loss / len(data_articles)\n",
        "        final_accuracy = correct_predictions / total_predictions\n",
        "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}, Final Training Accuracy: {final_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZK7zuSUz9Glx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training(data_articles, gradients, parameters, vocabulary, learning_rate,5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pxq5svUU2A5I",
        "outputId": "eee88129-7a99-4c67-bd91-617f936d75d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTED WORDS\n",
            "['Reuters', 'Reuters', 'Reuters', 'Into', 'Into', '(Reuters)', '(Reuters)', '(Reuters)']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 1, Average Loss: 3.5193, Final Training Accuracy: 0.0000\n",
            "Epoch 2, Average Loss: 3.4419, Final Training Accuracy: 0.0000\n",
            "Epoch 3, Average Loss: 3.2948, Final Training Accuracy: 0.0556\n",
            "Epoch 4, Average Loss: 3.1736, Final Training Accuracy: 0.0556\n",
            "Epoch 5, Average Loss: 3.1560, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'Pullback', 'Pullback', 'Pullback', 'Pullback', 'Pullback', 'Pullback', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 6, Average Loss: 3.2163, Final Training Accuracy: 0.0000\n",
            "Epoch 7, Average Loss: 3.1438, Final Training Accuracy: 0.0000\n",
            "Epoch 8, Average Loss: 3.2319, Final Training Accuracy: 0.0000\n",
            "Epoch 9, Average Loss: 3.1317, Final Training Accuracy: 0.0000\n",
            "Epoch 10, Average Loss: 3.2043, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Reuters', 'Into', 'Into', 'Pullback', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 11, Average Loss: 3.0391, Final Training Accuracy: 0.0000\n",
            "Epoch 12, Average Loss: 3.0701, Final Training Accuracy: 0.0000\n",
            "Epoch 13, Average Loss: 3.0088, Final Training Accuracy: 0.0000\n",
            "Epoch 14, Average Loss: 3.1285, Final Training Accuracy: 0.0000\n",
            "Epoch 15, Average Loss: 3.2132, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 16, Average Loss: 3.4764, Final Training Accuracy: 0.0000\n",
            "Epoch 17, Average Loss: 3.7744, Final Training Accuracy: 0.0000\n",
            "Epoch 18, Average Loss: 4.0093, Final Training Accuracy: 0.0000\n",
            "Epoch 19, Average Loss: 4.1946, Final Training Accuracy: 0.0000\n",
            "Epoch 20, Average Loss: 4.3464, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 21, Average Loss: 4.4751, Final Training Accuracy: 0.0000\n",
            "Epoch 22, Average Loss: 4.5868, Final Training Accuracy: 0.0000\n",
            "Epoch 23, Average Loss: 4.6854, Final Training Accuracy: 0.0000\n",
            "Epoch 24, Average Loss: 4.7732, Final Training Accuracy: 0.0000\n",
            "Epoch 25, Average Loss: 4.8520, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 26, Average Loss: 4.9233, Final Training Accuracy: 0.0000\n",
            "Epoch 27, Average Loss: 4.9880, Final Training Accuracy: 0.0000\n",
            "Epoch 28, Average Loss: 5.0473, Final Training Accuracy: 0.0000\n",
            "Epoch 29, Average Loss: 5.1017, Final Training Accuracy: 0.0000\n",
            "Epoch 30, Average Loss: 5.1520, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 31, Average Loss: 5.1988, Final Training Accuracy: 0.0000\n",
            "Epoch 32, Average Loss: 5.2425, Final Training Accuracy: 0.0000\n",
            "Epoch 33, Average Loss: 5.2834, Final Training Accuracy: 0.0000\n",
            "Epoch 34, Average Loss: 5.3220, Final Training Accuracy: 0.0000\n",
            "Epoch 35, Average Loss: 5.3585, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 36, Average Loss: 5.3932, Final Training Accuracy: 0.0000\n",
            "Epoch 37, Average Loss: 5.4263, Final Training Accuracy: 0.0000\n",
            "Epoch 38, Average Loss: 5.4578, Final Training Accuracy: 0.0000\n",
            "Epoch 39, Average Loss: 5.4881, Final Training Accuracy: 0.0000\n",
            "Epoch 40, Average Loss: 5.5172, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 41, Average Loss: 5.5453, Final Training Accuracy: 0.0000\n",
            "Epoch 42, Average Loss: 5.5724, Final Training Accuracy: 0.0000\n",
            "Epoch 43, Average Loss: 5.5986, Final Training Accuracy: 0.0000\n",
            "Epoch 44, Average Loss: 5.6240, Final Training Accuracy: 0.0000\n",
            "Epoch 45, Average Loss: 5.6486, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 46, Average Loss: 5.6726, Final Training Accuracy: 0.0000\n",
            "Epoch 47, Average Loss: 5.6959, Final Training Accuracy: 0.0000\n",
            "Epoch 48, Average Loss: 5.7187, Final Training Accuracy: 0.0000\n",
            "Epoch 49, Average Loss: 5.7408, Final Training Accuracy: 0.0000\n",
            "Epoch 50, Average Loss: 5.7624, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 51, Average Loss: 5.7836, Final Training Accuracy: 0.0000\n",
            "Epoch 52, Average Loss: 5.8042, Final Training Accuracy: 0.0000\n",
            "Epoch 53, Average Loss: 5.8244, Final Training Accuracy: 0.0000\n",
            "Epoch 54, Average Loss: 5.8441, Final Training Accuracy: 0.0000\n",
            "Epoch 55, Average Loss: 5.8634, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 56, Average Loss: 5.8823, Final Training Accuracy: 0.0000\n",
            "Epoch 57, Average Loss: 5.9009, Final Training Accuracy: 0.0000\n",
            "Epoch 58, Average Loss: 5.9190, Final Training Accuracy: 0.0000\n",
            "Epoch 59, Average Loss: 5.9368, Final Training Accuracy: 0.0000\n",
            "Epoch 60, Average Loss: 5.9542, Final Training Accuracy: 0.0000\n",
            "PREDICTED WORDS\n",
            "['wall', 'reuters', 'Tech', 'Reuters', 'Into', 'Wall', 'Bears', 'Pullback']\n",
            "Target words\n",
            "['pullback', 'reflects', 'tech', 'blowout', 'none', 'business', 'reuters', 'wall']\n",
            "Epoch 61, Average Loss: 5.9713, Final Training Accuracy: 0.0000\n",
            "Epoch 62, Average Loss: 5.9880, Final Training Accuracy: 0.0000\n",
            "Epoch 63, Average Loss: 6.0045, Final Training Accuracy: 0.0000\n",
            "Epoch 64, Average Loss: 6.0206, Final Training Accuracy: 0.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-545-448b9377abdb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-544-bb10a0eba090>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(mini_batches, gradients, parameters, vocabulary, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Create true labels (next words) for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-540-a4dba175f000>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(indices, word_embeddings, parameters)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# print(parameters['bh'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mh_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W_h'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W_e'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'es'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bh'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# print(\"HREC\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# print(h_rec.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}