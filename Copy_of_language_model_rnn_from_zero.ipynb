{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GNByOTGFntOtfHmtiIdQkSlkZ7GFbuAj",
      "authorship_tag": "ABX9TyObsvsDq2UdcRmXEuapS2/I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigoromanguzman/Actividades_Aprendizaje-/blob/main/Copy_of_language_model_rnn_from_zero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "8bHMJ7LQ3WOn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Utility libraries\n",
        "import random\n",
        "import re\n",
        "\n",
        "\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcruplZaz2cs",
        "outputId": "3f45a8a2-0d81-4f90-ccde-3834054154de"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_s_path = \"/content/drive/MyDrive/newsSpace\""
      ],
      "metadata": {
        "id": "5USCIixw0paW"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function for detecting urls\n",
        "def is_url(s):\n",
        "    # A simple regex to check for a basic URL structure\n",
        "    return re.match(r'https?://', s) is not None\n"
      ],
      "metadata": {
        "id": "HZWg_1WuDAfh"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will separate the batches as the elements from an individual article\n",
        "data_articles = []\n",
        "# Lets collect all the unique ocurrences of each word\n",
        "# to create the dictionary\n",
        "vocabulary = set([])\n",
        "num_articles = 100\n",
        "counter = 0\n",
        "with open(news_s_path, encoding='ISO-8859-1') as f:\n",
        "    while (counter<num_articles):\n",
        "      line = f.readline()\n",
        "      if not line:\n",
        "          break\n",
        "      data_line = line.strip().split()\n",
        "      url_index = next((i for i, item in enumerate(data_line) if is_url(item)), None)\n",
        "      # We will get only the articles which have a url\n",
        "      if url_index is not None:\n",
        "        # We take after the url\n",
        "        article = line.strip().split()[url_index+1::]\n",
        "        [vocabulary.add(i) for i in article]\n",
        "        data_articles.append(article)\n",
        "      counter += 1\n",
        "print(\"Vocabulary size -> \", len(vocabulary))\n",
        "# print(data_articles[:100])\n",
        "# print(list(vocabulary)[:200])\n"
      ],
      "metadata": {
        "id": "xt4wscHo214n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03676cd-ed7b-4ef6-ed9a-d50fafe8bce9"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size ->  1448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6lsEiAJf1RF"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into training and test\n",
        "test_percentage = 0.2\n",
        "random.shuffle(data_articles)\n",
        "split_point = int(len(data_articles)*test_percentage)\n",
        "training_set = data_articles[split_point::]\n",
        "test_set = data_articles[:split_point:]"
      ],
      "metadata": {
        "id": "tSQMCFvbDjJM"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embedding initialization with random values\n",
        "mean = 0\n",
        "std_dev = 0.01\n",
        "vocab_size = len(vocabulary)\n",
        "embedding_size = 150 #Hardcoded value for the size of the vectors\n",
        "word_embeddings = np.random.normal(mean, std_dev, size=(vocab_size, embedding_size))\n",
        "hidden_size = 500  # Size of the hidden state vectors\n",
        "\n",
        "# Create a dictionary to map words to their embedding vectors\n",
        "word_to_embedding = {}\n",
        "for i, word in enumerate(vocabulary):\n",
        "    word_to_embedding[word] = word_embeddings[i]"
      ],
      "metadata": {
        "id": "enODZ5lLI3qC"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights randomly\n",
        "W_e = np.random.normal(mean, std_dev, size=(vocab_size, embedding_size))\n",
        "W_h = np.random.normal(mean, std_dev, size=(hidden_size,hidden_size))\n",
        "W_y = np.random.normal(mean, std_dev, size=(hidden_size,vocab_size))# Hidden to output\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((vocab_size, 1))  # Output bias\n"
      ],
      "metadata": {
        "id": "l0yWqAXzbNHq"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "40ncTkkUWgX9"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoded vocabulary\n",
        "vocabulary_list = list(vocabulary)\n",
        "\n",
        "vocabulary_list.sort()  # Sorting to ensure consistent indexing\n",
        "\n",
        "# Create a dictionary that maps words to indices based on the sorted order\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "word_to_index['<UNK>'] = len(vocabulary)  # Add '<UNK>' token with a new index"
      ],
      "metadata": {
        "id": "6A0YL2tBVh3h"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_indices(sentence, word_to_index):\n",
        "    return [word_to_index.get(word, word_to_index['<UNK>']) for word in sentence.split()]"
      ],
      "metadata": {
        "id": "X5sf0WJdmKdG"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return np.exp(x)/sum(np.exp(x))"
      ],
      "metadata": {
        "id": "4L30yXQBIAkb"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tahnh(x):\n",
        "  return np.tanh(x)"
      ],
      "metadata": {
        "id": "U0UwotcsMbGB"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input the probabilies and the true value\n",
        "def cross_entropy(y_probs,y_true):\n",
        "  return -np.sum(y_true * np.log(y_probs))"
      ],
      "metadata": {
        "id": "NKUkOR4mM-Lx"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input the sentnece as thie index representation\n",
        "# parameters: dictionary with keys -> W_e, W_h, W_y, bh, by\n",
        "def forward_pass(indices, word_embeddings,parameters):\n",
        "    h_prev = np.zeros((hidden_size, 1))  # Initial hidden state\n",
        "\n",
        "    states = {'es': {},'hs': {},'ys': {},'ps': {}}\n",
        "    states['hs'][-1] = np.copy(h_prev)\n",
        "\n",
        "    # Forward pass\n",
        "    for t in range(len(indices)):\n",
        "        states['es'][t] = word_embeddings[indices[t]]  # Embedding vector for current input word\n",
        "        h_rec = np.dot(parameters['W_h'], h_prev)+ np.dot(parameters['W_e'], states['es'][t]) + parameters['bh']\n",
        "        states['hs'][t] = tahnh(h_rec)  # Hidden state\n",
        "        states['ys'][t] = np.dot(parameters['W_y'].T, states['hs'][t]) + parameters['by']  # Unnormalized log probabilities for next words\n",
        "        states['ps'][t] = softmax(states['ys'][t])  # Probabilities for next words\n",
        "        h_prev = states['hs'][t]  # Pass the current hidden state to the next time step\n",
        "    return states"
      ],
      "metadata": {
        "id": "7KgFU-K4HSrq"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "# outputs -> dictionary of dictionaries: es, hs, ys, ps\n",
        "# parameters -> dictionary with: W_h, W_y, bh, by\n",
        "\n",
        "def backward_pass(inputs,outputs, parameters):\n",
        "    gradients = {\n",
        "        'dW_e': np.zeros_like(W_e),\n",
        "        'dW_h': np.zeros_like(parameters['W_h']),\n",
        "        'dW_y': np.zeros_like(parameters['W_y']),\n",
        "        'dbh': np.zeros_like(parameters['bh']),\n",
        "        'dby': np.zeros_like(parameters['by'])\n",
        "    }\n",
        "    dh_next = np.zeros_like(outputs['hs'][0])\n",
        "    for t in reversed(range(len(inputs)-1)):\n",
        "        dy = np.copy(outputs['ps'][t])  # Copy softmax probabilities\n",
        "        true_label_index = inputs[t + 1] if t < len(inputs) - 1 else inputs[t]\n",
        "        dy[true_label_index] -= 1\n",
        "\n",
        "        temp_product = np.dot(dy, outputs['hs'][t].T)  # Results in shape (1448, 500)\n",
        "        gradients['dW_y'] += temp_product.T\n",
        "\n",
        "        gradients['dby'] += dy\n",
        "        dh = np.dot(parameters['W_y'].T, dy) + dh_next  # Backprop into h\n",
        "        dh_rec = (1 - outputs['hs'][t] * outputs['hs'][t]) * dh  # Backprop through tanh nonlinearity\n",
        "        gradients['dbh'] += dh_rec\n",
        "        gradients['dW_h'] += np.dot(dh_rec, outputs['hs'][t-1].T)\n",
        "        gradients['dW_e'] += np.dot(dh_rec, outputs['es'][t].T)\n",
        "        dh_next = np.dot(parameters['W_h'].T, dh_rec)\n",
        "\n",
        "    for gradientKey in gradients:\n",
        "        np.clip(gradients[gradientKey], -5, 5, out=gradients[gradientKey])  # Clip to mitigate exploding gradients\n",
        "    return gradients\n"
      ],
      "metadata": {
        "id": "h_c6LcuMQEEJ"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update weights\n",
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    # parameters and gradients are dictionaries with the same keys: 'W_e', 'W_h', 'W_y', 'bh', 'by'\n",
        "    for key in parameters.keys():\n",
        "        parameters[key] -= learning_rate * gradients['d' + key]"
      ],
      "metadata": {
        "id": "OXxWrosQALZX"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# And we have our parameters 'W_e', 'W_h', 'W_y', 'bh', 'by'\n",
        "parameters = {\n",
        "    'W_e': W_e,\n",
        "    'W_h': W_h,\n",
        "    'W_y': W_y,\n",
        "    'bh': bh,\n",
        "    'by': by\n",
        "}\n",
        "def initialize_gradients(parameters):\n",
        "    gradients = {}\n",
        "    for key in parameters.keys():\n",
        "        gradients['d' + key] = np.zeros_like(parameters[key])\n",
        "    return gradients\n",
        "\n",
        "# Usage\n",
        "gradients = initialize_gradients(parameters)\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "WiiTahqQ68kJ"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def training(mini_batches,gradients,parameters,vocabulary,learning_rate):\n",
        "\n",
        "  total_loss = 0\n",
        "  correct_predictions = 0\n",
        "  total_predictions = 0\n",
        "\n",
        "  for batch_idx, sentence in enumerate(data_articles, start=1):\n",
        "      indices = [word_to_index.get(word, word_to_index['<UNK>']) for word in sentence]  # Tokenize the sentence\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = forward_pass(indices, word_embeddings, parameters)\n",
        "\n",
        "      # Create true labels (next words) for this batch\n",
        "      true_labels = indices[1:]  # Assuming the next word is the true label for each word in the sentence\n",
        "      print(\"Passed forward\")\n",
        "      # Backward pass\n",
        "      gradients = backward_pass(indices, outputs, parameters)\n",
        "      print(\"passed gradient\")\n",
        "\n",
        "      # Update parameters\n",
        "      update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "      # Calculate and print loss\n",
        "      loss = cross_entropy(outputs['ps'], true_labels)\n",
        "      total_loss += loss\n",
        "      print(f\"Batch {batch_idx}, Loss: {loss}\")\n",
        "\n",
        "      # Calculate accuracy for this batch\n",
        "      predicted_words = [vocabulary_list[np.argmax(output)] for output in outputs['ps']]\n",
        "      correct_predictions += sum([1 for predicted, true in zip(predicted_words, true_labels) if predicted == true])\n",
        "      total_predictions += len(true_labels)\n",
        "\n",
        "      # Calculate and print accuracy on the training data at specified intervals\n",
        "      if batch_idx % 20 == 0:\n",
        "          accuracy = correct_predictions / total_predictions\n",
        "          print(f\"Batch {batch_idx}, Training Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "  # Print average loss and final training accuracy\n",
        "  avg_loss = total_loss / len(data_articles)\n",
        "  final_accuracy = correct_predictions / total_predictions\n",
        "  print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "  print(f\"Final Training Accuracy: {final_accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ZK7zuSUz9Glx"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training(data_articles, gradients, parameters, vocabulary, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "Pxq5svUU2A5I",
        "outputId": "a2ecd453-0a9a-4a2c-90c3-011a384e4f2a"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passed forward\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-b21294fe2058>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-151-790adb5cf632>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(mini_batches, gradients, parameters, vocabulary, learning_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Passed forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"passed gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-148-ad8872d464b2>\u001b[0m in \u001b[0;36mbackward_pass\u001b[0;34m(inputs, outputs, parameters)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dW_y'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtemp_product\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dby'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdh_next\u001b[0m  \u001b[0;31m# Backprop into h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdh_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdh\u001b[0m  \u001b[0;31m# Backprop through tanh nonlinearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1448,1) doesn't match the broadcast shape (1448,1448)"
          ]
        }
      ]
    }
  ]
}